{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8780816,"sourceType":"datasetVersion","datasetId":3442424},{"sourceId":247563,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":211587,"modelId":233277}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport os\nfrom torch import nn\nimport torchvision\nfrom torchvision import transforms\nimport  torchvision.models as models\nfrom torchvision.transforms import ToTensor,Compose,Normalize,Resize,CenterCrop,ColorJitter\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import random_split\nfrom PIL import Image\nfrom torch import nn\nimport pathlib\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torchsummary import summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing the dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n# Download latest version\npath = kagglehub.dataset_download(\"phucthaiv02/butterfly-image-classification\")\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataroot = path         #directory for the dataset\nN_EPOCHS = 100          #No. of epochs to train\nBATCH_SIZE = 128        #batch_size\nz_dim = 100             #Latent dimensions (dimension of the random noise to be fed to the generator)\nN_critic = 1            #No. of times for the critic to be trained per generator iteration\nImg_channels = 3            \nInput_Shape = [3,128,128]\nHidden_dims = 64        #No. of Hidden channels \nlr = 1e-4               #learning rate\nbetas = (0.5,0.999)     #beta values for adam optmizer\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() ) else \"cpu\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def weights_init(m):\n    \n    \"\"\" Initializes the parameters of the model\"\"\"\n    \n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n   \n    \"\"\"The critic network for the DC Gan\"\"\"\n\n    def __init__(self, Input_channels = Img_channels):\n        super(Discriminator, self).__init__()\n\n        self.main = nn.Sequential(nn.Conv2d(in_channels=Input_channels, out_channels = Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),    #[3, 128, 128] --> [Hidden_dims, 64, 64]\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.BatchNorm2d(Hidden_dims),\n\n        nn.Conv2d(in_channels = Hidden_dims, out_channels = Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),   #[Hidden_dims, 64, 64] --> [Hidden_dims, 32, 32]   \n        nn.LeakyReLU(0.2, inplace = True),\n        nn.BatchNorm2d(Hidden_dims),\n\n        nn.Conv2d(in_channels = Hidden_dims, out_channels = 2*Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),     #[Hidden_dims, 32, 32] --> [2*Hidden_dims, 16, 16]   \n        nn.LeakyReLU(0.2, inplace = True),\n        nn.BatchNorm2d(2*Hidden_dims),\n\n        nn.Conv2d(in_channels = 2*Hidden_dims, out_channels = 4*Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),       #[2*Hidden_dims, 16, 16] --> [4*Hidden_dims, 8, 8]   \n        nn.LeakyReLU(0.2, inplace = True),\n        nn.BatchNorm2d(4*Hidden_dims),\n\n        nn.Conv2d(in_channels = 4*Hidden_dims, out_channels = 8*Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),       #[4*Hidden_dims, 16, 16] --> [8*Hidden_dims, 4, 4]  \n        nn.LeakyReLU(0.2, inplace = True),\n        nn.BatchNorm2d(8*Hidden_dims),\n        \n        nn.Conv2d(in_channels = 8*Hidden_dims, out_channels = 1, kernel_size = 4, stride = 1, padding = 0, bias = False),       #[8*Hidden_dims, 4, 4] --> [1, 1, 1]  \n        nn.Sigmoid())\n\n    def forward(self,x):\n        return self.main(x)\n        \n\n\n\n\n\n        \n\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n\n    \"\"\" Generator Network for DC GAN with Transpose Convulutional layers for upsampling\"\"\"\n\n    def __init__(self, z = z_dim, Output_channels = Img_channels):\n        super(Generator, self).__init__()\n        \n        self.main = nn.Sequential(nn.ConvTranspose2d(in_channels = z_dim, out_channels = 8*Hidden_dims, kernel_size = 4, stride = 1, padding = 0, bias = False),    #[z_dim, 1, 1] --> [8*Hidden_dims, 4, 4]\n        nn.BatchNorm2d(8*Hidden_dims),\n        nn.LeakyReLU(0.2,True),\n        \n        nn.ConvTranspose2d(in_channels = 8*Hidden_dims, out_channels = 4*Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),      #[8*Hidden_dims, 4, 4] --> [4*Hidden_dims, 8, 8]\n        nn.BatchNorm2d(4*Hidden_dims),\n        nn.LeakyReLU(0.2, True),\n        \n        nn.ConvTranspose2d(in_channels = 4*Hidden_dims, out_channels = 2*Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),      #[4*Hidden_dims, 8, 8] --> [2*Hidden_dims, 16, 16]\n        nn.BatchNorm2d(2*Hidden_dims),\n        nn.LeakyReLU(0.2, True),\n        \n        nn.ConvTranspose2d(in_channels = 2*Hidden_dims, out_channels = Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),        #[2*Hidden_dims, 16, 16] --> [Hidden_dims, 32, 32]\n        nn.BatchNorm2d(Hidden_dims),\n        nn.LeakyReLU(0.2, True),\n\n        nn.ConvTranspose2d(in_channels = Hidden_dims, out_channels = Hidden_dims, kernel_size = 4, stride = 2, padding = 1, bias = False),      #[Hidden_dims, 32, 32] --> [Hidden_dims, 64, 64]\n        nn.BatchNorm2d(Hidden_dims),\n        nn.LeakyReLU(0.2, True),\n        \n        nn.ConvTranspose2d(in_channels = Hidden_dims, out_channels = Output_channels, kernel_size = 4, stride = 2, padding = 1, bias = False),      #[Hidden_dims, 64, 64] --> [3, 128, 128]\n        nn.Tanh())\n    \n    def forward(self,x):\n        return self.main(x)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator2(nn.Module):\n\n    \"\"\" Generator Network for DC GAN with Upsampling(Nearest2d)+ Convulutional layers  for upsampling\"\"\"\n\n    def __init__(self, z = z_dim, Output_channels = Img_channels):\n        super(Generator2, self).__init__()\n        \n        self.main = nn.Sequential(nn.ConvTranspose2d(in_channels = z_dim, out_channels = 4*Hidden_dims, kernel_size = 7, stride = 1, padding = 1, bias = False),        #[z_dims, 1, 1] --> [4*Hidden_dims, 5, 5]        \n        nn.LayerNorm([4*Hidden_dims,5,5]),\n        nn.LeakyReLU(0.2,True),\n\n        nn.UpsamplingNearest2d(scale_factor=2),     #[4*Hiden_dims, 5, 5] --> [4*Hidden_dims, 10, 10]\n        nn.Conv2d(in_channels=4*Hidden_dims,out_channels=2*Hidden_dims,kernel_size=7,stride=1,padding=2),       #[4*Hiden_dims, 10, 10] --> [2*Hidden_dims, 8, 8]\n        nn.LayerNorm([2*Hidden_dims,8,8]),\n        nn.LeakyReLU(0.2,True),\n        \n        nn.UpsamplingNearest2d(scale_factor=2),     #[2*Hidden_dims, 8, 8] --> [2*Hidden_dims, 16, 16]\n        nn.Conv2d(in_channels=2*Hidden_dims,out_channels=Hidden_dims,kernel_size=5,stride=1,padding=2),     #[2*Hidden_dims, 16, 16] --> [Hidden_dims, 16, 16]\n        nn.LayerNorm([Hidden_dims,16,16]),\n        nn.LeakyReLU(0.2,True),\n\n        nn.UpsamplingNearest2d(scale_factor=2),     #[Hidden_dims, 16, 16] --> [Hidden_dims, 32, 32]\n        nn.Conv2d(in_channels=Hidden_dims,out_channels=Hidden_dims,kernel_size=5,stride=1,padding=2),       #[Hidden_dims, 32, 32] --> [Hidden_dims, 32, 32]\n        nn.LayerNorm([Hidden_dims,32,32]),\n        nn.LeakyReLU(0.2,True),\n\n        nn.UpsamplingNearest2d(scale_factor=2),     #[Hidden_dims, 32, 32] --> [Hidden_dims, 64, 64]\n        nn.Conv2d(in_channels=Hidden_dims,out_channels=Hidden_dims,kernel_size=3,stride=1,padding=1),       #[Hidden_dims, 64, 64] --> [Hidden_dims, 64, 64]\n        nn.LayerNorm([Hidden_dims,64,64]),\n        nn.LeakyReLU(0.5,True),\n        \n        nn.UpsamplingNearest2d(scale_factor=2),    #[Hidden_dims, 64, 64] --> [Hidden_dims, 128, 128]\n        nn.Conv2d(in_channels=Hidden_dims,out_channels=Output_channels,kernel_size=3,stride=1,padding=1),       #[Hidden_dims, 128, 128] --> [3, 128, 128]\n        nn.Tanh())\n    \n    def forward(self,x):\n        return self.main(x)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initializing the Generator and critic network and initialing their weights.\ngenerator = Generator2().to(device)\ndiscriminator = Discriminator().to(device)\nweights_init(generator)\nweights_init(discriminator)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(discriminator,input_size=(3,128,128))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(generator,input_size=(100,1,1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_noise(size = z_dim, batch_size = BATCH_SIZE):\n\n    \"\"\"Samples latent variable/Noise of size [Batch_size, z_dim, 1, 1] from a normal distribution\"\"\"\n\n    return torch.randn(batch_size, z_dim, 1, 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#optimizer for the generator\nopt_gen = torch.optim.Adam(generator.parameters(),lr = 0.001,betas=betas)    \n#optimizer for the discriminator\nopt_disc = torch.optim.Adam(discriminator.parameters(),lr = 0.0001,betas = betas)       \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Differentiable Augmentations\n## Taken from https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_pytorch.py\ndef DiffAugment(x, policy ='color,translation,cutout', channels_first = True):\n    if policy:\n        if not channels_first:\n            x = x.permute(0, 3, 1, 2)\n        for p in policy.split(','):\n            for f in AUGMENT_FNS[p]:\n                x = f(x)\n        if not channels_first:\n            x = x.permute(0, 2, 3, 1)\n        x = x.contiguous()\n    return x\n\n\ndef rand_brightness(x):\n    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device = x.device) - 0.5)\n    return x\n\n\ndef rand_saturation(x):\n    x_mean = x.mean(dim = 1, keepdim = True)\n    x = (x - x_mean) * (torch.rand(x. size(0), 1, 1, 1,\n                                   dtype = x.dtype, device = x.device) * 2) + x_mean\n    return x\n\n\ndef rand_contrast(x):\n    x_mean = x.mean(dim=[1, 2, 3], keepdim = True)\n    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1,\n                                   dtype=x.dtype, device = x.device) + 0.5) + x_mean\n    return x\n\n\ndef rand_translation(x, ratio = 0.125):\n    shift_x, shift_y = int(x.size(2) * ratio +\n                           0.5), int(x.size(3) * ratio + 0.5)\n    translation_x = torch.randint(-shift_x, shift_x + 1,\n                                  size=[x.size(0), 1, 1], device = x.device)\n    translation_y = torch.randint(-shift_y, shift_y + 1,\n                                  size=[x.size(0), 1, 1], device = x.device)\n    grid_batch, grid_x, grid_y = torch.meshgrid(\n        torch.arange(x.size(0), dtype=torch.long, device = x.device),\n        torch.arange(x.size(2), dtype=torch.long, device = x.device),\n        torch.arange(x.size(3), dtype=torch.long, device = x.device),\n    )\n    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n    x_pad = torch.nn.functional.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n    x = x_pad.permute(0, 2, 3, 1).contiguous()[\n        grid_batch, grid_x, grid_y].permute(0, 3, 1, 2).contiguous()\n    return x\n\n\ndef rand_cutout(x, ratio = 0.5):\n    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n    offset_x = torch.randint(0, x.size(\n        2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n    offset_y = torch.randint(0, x.size(\n        3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n    grid_batch, grid_x, grid_y = torch.meshgrid(\n        torch.arange(x.size(0), dtype = torch.long, device = x.device),\n        torch.arange(cutout_size[0], dtype = torch.long, device = x.device),\n        torch.arange(cutout_size[1], dtype = torch.long, device = x.device),\n    )\n    grid_x = torch.clamp(grid_x + offset_x -\n                         cutout_size[0] // 2, min=0, max = x.size(2) - 1)\n    grid_y = torch.clamp(grid_y + offset_y -\n                         cutout_size[1] // 2, min=0, max = x.size(3) - 1)\n    mask = torch.ones(x.size(0), x.size(2), x.size(3),\n                      dtype = x.dtype, device = x.device)\n    mask[grid_batch, grid_x, grid_y] = 0\n    x = x * mask.unsqueeze(1)\n    return x\n\n\nAUGMENT_FNS = {\n    'color': [rand_brightness, rand_saturation, rand_contrast],\n    'translation': [rand_translation],\n    'cutout': [rand_cutout],\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#transformations to be applied to the input images such as resizing to 128 x 128 and converting them to tensors.\nTransforms = transforms.Compose([transforms.ToTensor(),transforms.Resize((128,128)),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])    \nimg_dataset = torchvision.datasets.ImageFolder(root  =dataroot,transform = Transforms)      \ndataloader = DataLoader(img_dataset,batch_size=BATCH_SIZE,shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_DC_gan(\n        Device = device,\n        gen_net = generator,\n        disc_net = discriminator,\n        Dataloader = dataloader,\n        opt_gen = opt_gen,\n        opt_disc = opt_disc,\n        noise_sampler = sample_noise,\n        D_losses = [],\n        G_losses = [],\n        EPOCHS = N_EPOCHS,\n        N_critic = N_critic):\n    \n    step = 0\n    EPS = 1e-9  # for Numerical stability\n\n    for epoch in range(EPOCHS):\n\n        for (X,y) in tqdm(Dataloader):\n            X = X.to(device)\n            curr_batch_size = X.shape[0]\n\n            #training discriminator\n            for _ in range(N_critic):\n                #real images\n                opt_disc.zero_grad()\n                d_real = discriminator(DiffAugment(X))\n                loss_d_real = -1*torch.log(d_real+EPS).mean()\n                loss_d_real.backward()\n                #fake images\n                noise = noise_sampler(batch_size=curr_batch_size)\n                fake = generator(noise)\n                d_fake = discriminator(DiffAugment(fake).detach())\n                loss_d_fake = -1*torch.log(1-d_fake+EPS).mean()\n                loss_d_fake.backward(retain_graph=True) \n                opt_disc.step()\n                \n            #training generator\n            opt_gen.zero_grad()\n            g_fake = discriminator(DiffAugment(fake))\n            loss_g = (-1*torch.log(g_fake+EPS)).mean()\n            loss_g.backward(retain_graph=True)\n            opt_gen.step()\n            \n            if step %50 == 0:\n                print(f\"epoch:{epoch+1} iter{step} disc_loss:{(loss_d_fake+loss_d_real)} gen_loss:{loss_g}\")\n        \n            if step % 200 == 0:\n                with torch.no_grad():\n                # Use the first image from fake_images instead of generating new ones\n                    fake_images = fake[0].unsqueeze(0).cpu()  # Take the first image and add batch dimension\n                    # Ensure the image is in the range [0, 1]\n                    fake = (fake + 1) / 2.0  # Transform from [-1, 1] to [0, 1]\n                    img = torchvision.utils.make_grid(fake, normalize=False)\n                    img_np = img.detach().permute(1, 2, 0).numpy()  # Add detach() here\n                    plt.figure(figsize=(8, 8))\n                    plt.imshow(img_np)\n                    plt.axis('off')\n                    plt.title(f\"Epoch {epoch+1}, iter {step}\")\n                    save_dir = \"generated_images\"\n                    os.makedirs(save_dir,exist_ok= True)\n                    save_path = os.path.join(save_dir, f'generated_image_epoch_{epoch+1}_batch_{step+1}.png')\n                    plt.savefig(save_path)\n                    plt.close()\n            step +=1\n                \n        G_losses.append(loss_g.item())\n        D_losses.append(loss_d_fake.item()+loss_d_real.item())\n    return G_losses,D_losses\n            ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##plotting loss curves\ngenerator_loss,discriminator_loss = train_DC_gan(N_critic = 1)\nplt.plot(generator_loss)\nplt.plot(discriminator_loss)\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"loss\")\nplt.legend([\"generator\",\"discriminator\"])\nplt.title(f\"N_critic = {N_critic}\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_images(save_path,generator = generator, sample_noise = sample_noise, N_samples = 100,title = None):\n\n    \"\"\"Plot a grid a fake images generated by the generator\"\"\"\n\n    z = sample_noise(batch_size = N_samples)      # sample random noise\n    images = generator(z).detach().cpu()          # generate fake images \n    img = torchvision.utils.make_grid(images, nrow = 10, normalize = False)\n    img = img.permute(1, 2, 0).numpy()            # convert tensors to numpy array\n    img = (img +1)/2.0                            # Transform from [-1, 1] to [0, 1]\n    plt.figure(figsize = (10,10))\n    plt.axis('off')\n    if title:\n        plt.title(title)\n    plt.imshow(img)\n    plt.savefig(save_path)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"                                ##Frechet Inception Distance       ","metadata":{}},{"cell_type":"code","source":"from torchvision.models import inception_v3\n\npreprocess = transforms.Compose([       # Define preprocessing steps for Inception v3 input\n    transforms.Resize((299, 299)),  # Resize images to Inception v3 input size\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats ie we will also preprocess the data in same way as the original imagenet dataset that was used to train Inception v3\n])  \n\ndef prepare_inception_input(images):\n\n    \"\"\"Apply preprocessing to the input images\"\"\"\n\n    return preprocess(images)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_inception_model():\n\n    \"\"\"Load inception pretrained v3 model for extracting features\"\"\"\n    \n    model = inception_v3(pretrained=True, transform_input=False)  # Load pre-trained Inception v3 model without input transformation\n    model.fc = torch.nn.Identity()  # Removes the last fully connected layer\n    model.eval()  # Set the model to evaluation mode\n    return model.to(device)  # Move the model to the same device as our GAN\n\ninception_model = load_inception_model()  # Load and prepare the modified Inception v3 model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features(images):\n\n    \"\"\"returns features of size [2048] extracted from the inception model\"\"\"\n    \n    with torch.no_grad():\n        features = inception_model(images)\n    return features  # This will be a tensor of shape [batch_size, 2048]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_real_features(dataloader, num_images=1000):\n\n    \"\"\"return features of real images extracted from the inception model\"\"\"\n\n    real_features = []  # List to store features of real images\n    image_count = 0  # Counter for processed images\n\n    with torch.no_grad():  # Disable gradient computation for efficiency\n        for batch in dataloader:\n            images = batch[0].to(device)  # Move batch of images to the device (assuming batch[0] contains images)\n            batch_size = images.size(0)  # Get the current batch size\n            \n            if image_count + batch_size > num_images:\n                # If adding this batch would exceed num_images, only take what's needed\n                images = images[:num_images - image_count]\n            \n            preprocessed_images = prepare_inception_input(images)  # Preprocess images for Inception v3\n            batch_features = extract_features(preprocessed_images)  # Extract features using Inception v3\n            \n            real_features.append(batch_features)  # Add batch features to the list\n            \n            image_count += batch_features.size(0)  # Update the count of processed images\n            if image_count >= num_images:\n                break  # Stop if we've processed enough images\n\n    real_features_tensor = torch.cat(real_features, dim=0)  # Concatenate all features into a single tensor\n    return real_features_tensor[:num_images]  # Return exactly num_images features\n\n# Extract features from 1000 real images\nreal_features = extract_real_features(dataloader, num_images=9000)  # Extract features from 1000 real images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_and_extract_features(generator, num_samples=1000, batch_size=64):\n\n    \"\"\"return features of fake images extracted from the inception model\"\"\"\n\n    generated_features = []  # List to store features of generated images\n    num_batches = (num_samples + batch_size - 1) // batch_size  # Calculate number of batches needed\n\n    generator.eval()  # Set generator to evaluation mode\n    print(f\"Starting feature extraction for {num_samples} samples with batch size {batch_size}\")  # Print start of extraction process\n    with torch.no_grad():  # Disable gradient computation for efficiency\n        for batch_idx in range(num_batches):\n            current_batch_size = min(batch_size, num_samples - len(generated_features))  # Adjust batch size for last batch if needed\n            print(f\"Processing batch {batch_idx + 1}/{num_batches} with size {current_batch_size}\")  # Print current batch information\n            noise = torch.randn(current_batch_size, 100, 1, 1, device=device)  # Generate noise for input to generator\n            fake_images = generator(noise)  # Generate fake images\n            \n            preprocessed_images = prepare_inception_input(fake_images)  # Preprocess images for Inception v3\n            batch_features = extract_features(preprocessed_images)  # Extract features using Inception v3\n            \n            generated_features.append(batch_features)  # Add batch features to the list\n            print(f\"Extracted features shape: {batch_features.shape}\")  # Print shape of extracted features\n            \n            if len(generated_features) * batch_size >= num_samples:\n                print(f\"Reached target number of samples. Stopping extraction.\")  # Print when target samples reached\n                break  # Stop if we've generated enough samples\n    print(f\"Feature extraction completed. Total features extracted: {len(generated_features) * batch_size}\")  # Print completion of extraction process\n\n    generated_features_tensor = torch.cat(generated_features, dim=0)  # Concatenate all features into a single tensor\n    return generated_features_tensor[:num_samples]  # Return exactly num_samples features\n\n# Generate 1000 samples and extract their features\ngenerated_features = generate_and_extract_features(generator, num_samples=9000)  # Generate and extract features from 1000 fake images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate mean and covariance of real features\nreal_mean = torch.mean(real_features, dim=0)  # Calculate mean across all samples for each feature\nreal_cov = torch.cov(real_features.T)  # Calculate covariance matrix of features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate mean and covariance of generated features\ngenerated_mean = torch.mean(generated_features, dim=0)  # Calculate mean across all samples for each feature\ngenerated_cov = torch.cov(generated_features.T)  # Calculate covariance matrix of features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_frechet_inception_distance(real_mean, real_cov, generated_mean, generated_cov):\n    \"\"\"\n    Calculate the Fréchet Inception Distance (FID) between real and generated image features.\n    \n    Args:\n    real_mean (torch.Tensor): Mean of real image features.\n    real_cov (torch.Tensor): Covariance matrix of real image features.\n    generated_mean (torch.Tensor): Mean of generated image features.\n    generated_cov (torch.Tensor): Covariance matrix of generated image features.\n    \n    Returns:\n    float: The calculated FID score.\n    \"\"\"\n    \n    # Convert to numpy for scipy operations\n    real_mean_np = real_mean.cpu().numpy()  # Convert real mean to numpy array\n    real_cov_np = real_cov.cpu().numpy()  # Convert real covariance to numpy array\n    generated_mean_np = generated_mean.cpu().numpy()  # Convert generated mean to numpy array\n    generated_cov_np = generated_cov.cpu().numpy()  # Convert generated covariance to numpy array\n    \n    # Calculate squared L2 norm between means\n    mean_diff = np.sum((real_mean_np - generated_mean_np) ** 2)  # Compute squared difference between means\n    \n    # Calculate sqrt of product of covariances\n    covmean = scipy.linalg.sqrtm(real_cov_np.dot(generated_cov_np))  # Compute matrix square root\n    \n    # Check and correct imaginary parts if necessary\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real  # Take only the real part if result is complex\n    \n    # Calculate trace term\n    trace_term = np.trace(real_cov_np + generated_cov_np - 2 * covmean)  # Compute trace of the difference\n    \n    # Compute FID\n    fid = mean_diff + trace_term  # Sum up mean difference and trace term\n    \n    return fid  # Return FID as a Python float\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate FID for animal dataset using the above function\nfid_score = calculate_frechet_inception_distance(real_mean, real_cov, generated_mean, generated_cov)  # Compute FID score\nprint(f\"Fréchet Inception Distance (FID): {fid_score:.4f}\")  # Print the calculated FID score","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}